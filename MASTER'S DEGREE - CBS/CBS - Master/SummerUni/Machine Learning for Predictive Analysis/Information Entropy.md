Entropy measures the **randomness** or **disorders** in a system. In terms of data, we can define it as the randomness in the information we are processing. The higher the randomness, the higher the entropy. Hence, harder to conclude from that information

It is related to [[Decision Trees]]. 